# -*- coding: utf-8 -*-
"""Associasion_Rules_Groceries_Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w5BF7RdKb_ai9hUWbpihUqwkjpAMyVRe

# **Library Imports**
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install mlxtend --upgrade

import time
import tracemalloc
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth

"""# **Exploring Dataset**"""

# Load the dataset
data = pd.read_csv("groceries.csv")
# printing the shape of the dataset
data.shape

# printing the heading
data.head()

"""# **Visualizing The Dataset**"""

# Gather All Items of Each Transactions into Numpy Array
transaction = []
for i in range(0, data.shape[0]):
    for j in range(0, data.shape[1]):
        transaction.append(data.values[i,j])
# converting to numpy array
transaction = np.array(transaction)
#  Transform Them a Pandas DataFrame
df = pd.DataFrame(transaction, columns=["items"])
# Put 1 to Each Item For Making Countable Table, to be able to perform Group By
df["incident_count"] = 1
#  Delete NaN Items from Dataset
indexNames = df[df['items'] == "nan" ].index
df.drop(indexNames , inplace=True)
# Making a New Appropriate Pandas DataFrame for Visualizations
df_table = df.groupby("items").sum().sort_values("incident_count", ascending=False).reset_index()
#  Initial Visualizations (Top 10 most frequent items)
df_table.head(10).style.background_gradient(cmap='Greens')

"""The output shows that whole milk has been purchased more frequently than other products.

### Length of the visualizations
"""

len(df_table)

"""Which indicates that the dataset contains 169 unique items.

# **Data Pre-Processing**

Before getting the most frequent itemsets, we need to transform our dataset into a True – False matrix where rows are transactions and columns are products.

Possible cell values are:
*   **True –** the transaction contains the item
*   **False –** transaction does not contain the item
"""

transaction = []
for i in range(data.shape[0]):
    transaction.append([str(data.values[i,j]) for j in range(data.shape[1])])
# Creating the numpy array of the transactions
transaction = np.array(transaction)
# Initializing the transactions
trans_encoder = TransactionEncoder() # Instanciate the encoder
trans_encoder_matrix = trans_encoder.fit(transaction).transform(transaction)
trans_encoder_matrix = pd.DataFrame(trans_encoder_matrix, columns = trans_encoder.columns_)

# select top 80 items
first80 = df_table["items"].head(80).values
# Extract Top 80 ''''My system crashes when I run with all 169 items, therefore I extract the top 80 frequent used items''''
dataset = trans_encoder_matrix.loc[:,first80]
dataset

"""# **Helper Functions**"""

i = 0.001
def perform_rule_calculation(transact_items_matrix, rule_type="fpgrowth"):
    """
    desc: this function performs the association rule calculation
    @params:
        - transact_items_matrix: the transaction X Items matrix
        - rule_type:
                    - apriori or Growth algorithms (default="fpgrowth")

    @returns:
        - the matrix containing 3 columns:
            - support: support values for each combination of items
            - itemsets: the combination of items
            - number_of_items: the number of items in each combination of items

        - the excution time for the corresponding algorithm
        - the memory usages for the corresponding algorithm

    """
    total_execution = 0
    mem_usages = 0


    if(not rule_type == "fpgrowth"):
        tracemalloc.start()
        t1 = time.time()
        rule_items = apriori(transact_items_matrix,
                       min_support = i,
                       use_colnames = True)
        t2 = time.time()
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        mem_usage = (peak-current)/ (1024*1024)
        total_execution = (t2 - t1)

    else:
        tracemalloc.start()
        t1=time.time()
        rule_items = fpgrowth(transact_items_matrix,
                       min_support = i,
                       use_colnames = True)
        t2=time.time()
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        mem_usage = (peak-current)/ (1024*1024)
        total_execution = (t2 - t1)

    rule_items['number_of_items'] = rule_items['itemsets'].apply(lambda x: len(x))
    print(f"Implementation of the {rule_type} algorithm with Minimum Support: {i}, Frequent occuring items are:\n {rule_items}\n")

    return rule_items, total_execution, mem_usage

def compute_association_rule(rule_matrix, metric = "lift", min_thresh = 1):
    """
    @desc: Compute the final association rule
    @params:
        - rule_matrix: the corresponding algorithms matrix
        - metric: the metric to be used (default is lift)
        - min_thresh: the minimum threshold (default is 1)

    @returns:
        - rules: all the information for each transaction satisfying the given metric & threshold
    """
    rules = association_rules(rule_matrix,
                              metric = metric,
                              min_threshold = min_thresh)

    return rules

# Plot Lift Vs Coverage(confidence)
def plot_metrics_relationship(rule_matrix, col1, col2):
    """
    desc: shows the relationship between the two input columns
    @params:
        - rule_matrix: the matrix containing the result of a rule (apriori or Fp Growth)
        - col1: first column
        - col2: second column
    """
    fit = np.polyfit(rule_matrix[col1], rule_matrix[col2], 1)
    fit_funt = np.poly1d(fit)
    plt.plot(rule_matrix[col1], rule_matrix[col2], 'yo', rule_matrix[col1],
    fit_funt(rule_matrix[col1]))
    plt.xlabel(col1)
    plt.ylabel(col2)
    plt.title('{} vs {}'.format(col1, col2))

def compare_time_exec(algo1=list, alg2=list):
    """
    @desc: shows the execution time between two algorithms
    @params:
        - algo1: list containing the description of first algorithm, where

        - algo2: list containing the description of second algorithm, where
    """

    execution_times = [algo1[1], algo2[1]]
    algo_names = (algo1[0], algo2[0])
    y=np.arange(len(algo_names))

    plt.bar(y,execution_times,color=['orange', 'blue'])
    plt.xticks(y,algo_names)
    plt.xlabel('Algorithms')
    plt.ylabel('Time')
    plt.title(f"Execution Time (seconds) Comparison\n Fp-Growth: {algo1[1]:.2f}sec, Apriori: {algo2[1]:.2f}sec")
    plt.show()

def compare_memory_usage(algo1=list, alg2=list):
    """
    @desc: shows the memory usages between two algorithms
    @params:
        - algo1: list containing the description of first algorithm, where

        - algo2: list containing the description of second algorithm, where
    """

    mem_usages = [algo1[1], algo2[1]]
    algo_names = (algo1[0], algo2[0])
    y=np.arange(len(algo_names))

    plt.bar(y,mem_usages,color=['red', 'green'])
    plt.xticks(y,algo_names)
    plt.xlabel('Algorithms')
    plt.ylabel('Memory')
    plt.title(f"Memory Usage (MB) Comparison\n Fp-Growth: {algo1[1]:.2f}mb, Apriori: {algo2[1]:.2f}mb")
    plt.show()

"""# **Using Fp-Growth Algorithm**"""

fpgrowth_matrix, fp_growth_exec_time, fp_growth_mem_usage = perform_rule_calculation(dataset) # Run the algorithm

print("Fp Growth execution took: {} seconds".format(fp_growth_exec_time))
print("Fp Growth Memory Usage: {} MB".format(fp_growth_mem_usage))

"""### **Lift**"""

fp_growth_rule_lift = compute_association_rule(fpgrowth_matrix)
fp_growth_rule_lift

plot_metrics_relationship(fp_growth_rule_lift, col1 = 'lift', col2 = 'confidence')

"""### **Confidence**"""

fp_growth_rule = compute_association_rule(fpgrowth_matrix, metric = "confidence", min_thresh = 0.3)
fp_growth_rule.head()

"""# **Using Apriori Algorithm**"""

apriori_matrix, apriori_exec_time, apriori_mem_usage = perform_rule_calculation(dataset, rule_type="apriori")

print("Apriori Execution took: {} seconds".format(apriori_exec_time))
print("Apriori Memory Usage: {} MB".format(apriori_mem_usage))

"""### **Lift**"""

apriori_rule_lift = compute_association_rule(apriori_matrix)
apriori_rule_lift

plot_metrics_relationship(apriori_rule_lift, col1 = 'lift', col2 = 'confidence')

"""### **Confidence**"""

apripri_rule = compute_association_rule(apriori_matrix, metric = "confidence", min_thresh = 0.3)
apripri_rule.head()

"""# **Comparative Analysis**

### **Time Execution**
"""

algo1 = ['Fp Growth', fp_growth_exec_time]
algo2 = ['Apriori', apriori_exec_time]

compare_time_exec(algo1, algo2)

"""### **Memory Usages**"""

algo1 = ['Fp Growth', fp_growth_mem_usage]
algo2 = ['Apriori', apriori_mem_usage]

compare_memory_usage(algo1, algo2)

"""# **Implementing Algorithms with Different Minimum Support**

### **Fp-Growth**
"""

l = [0.001,0.003,0.005,0.01,0.05]
f = []
fm_usages = []
for i in l:
    fpgrowth_matrix, fp_growth_exec_time, fp_growth_mem_usage = perform_rule_calculation(dataset)
    f.append(fp_growth_exec_time)
    fm_usages.append(fp_growth_mem_usage)

print(f"Time Used:{f}")
print(f"Memory Used:{fm_usages}")

"""### **Apriori**"""

l = [0.001,0.003,0.005,0.01,0.05]
t = []
m_usages = []
for i in l:
    apriori_matrix, apriori_exec_time, apriori_mem_usage = perform_rule_calculation(dataset, rule_type="apriori")
    t.append(apriori_exec_time)
    m_usages.append(apriori_mem_usage)

print(f"Time Used:{t}")
print(f"Memory Used:{m_usages}")

"""# **Comparative Analysis with different minimum support**

### **Time Execution**
"""

n = 5
for i in range(n):
    algo1 = ['Fp Growth', f[i]]
    algo2 = ['Apriori', t[i]]
    print(f"For minimum support = {l[i]},")
    compare_time_exec(algo1, algo2)

"""### **Memory Usages**"""

n = 5
for i in range(n):
    algo1 = ['Fp Growth', fm_usages[i]]
    algo2 = ['Apriori', m_usages[i]]
    print(f"For minimum support = {l[i]},")
    compare_memory_usage(algo1, algo2)